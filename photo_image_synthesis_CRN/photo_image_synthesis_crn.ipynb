{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D,\\\n",
    "        AveragePooling2D, UpSampling2D, Lambda, Activation, merge, GlobalAveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from PIL import Image\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imsave\n",
    "import bcolz\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Open dataset\n",
    "x_train = bcolz.open('dataset3/x_train.bc')[:]\n",
    "y_train = bcolz.open('dataset3/y_train.bc')[:]\n",
    "\n",
    "x_validate = bcolz.open('dataset3/x_validate.bc')[:]\n",
    "y_validate = bcolz.open('dataset3/y_validate.bc')[:]\n",
    "\n",
    "#x_test = bcolz.open('dataset3/x_test.bc')[:]\n",
    "#y_test = bcolz.open('dataset3/y_test.bc')[:]\n",
    "\n",
    "output_shape = y_train[0].shape\n",
    "num_train = y_train.shape[0]\n",
    "num_validate = y_validate.shape[0]\n",
    "#num_test = y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_output(m, ln):\n",
    "    name = 'block' + str(ln) + '_conv2'\n",
    "    return m.get_layer(name).output\n",
    "\n",
    "def mean_squared_error(diff): \n",
    "    dims = list(range(1,K.ndim(diff)))\n",
    "    return K.expand_dims(K.sqrt(K.mean(diff**2, dims)), 0)\n",
    "\n",
    "def content_fn(x):\n",
    "    res = 0\n",
    "    n = len(layer_weights)\n",
    "    for i in range(n):\n",
    "        res += mean_squared_error(x[i]-x[i+n]) * layer_weights[i]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''NETWORK #1 - Generative network'''\n",
    "L = Input(x_train.shape[1:])\n",
    "sc = 64 #Scale factor\n",
    "\n",
    "#M0\n",
    "##Input layer - Downsample L\n",
    "L_down = AveragePooling2D(padding='same')(L)\n",
    "n = int(math.log(sc,2) - 1)\n",
    "for i in range(n):\n",
    "    L_down = AveragePooling2D(padding='same')(L_down) \n",
    "#Intermediate layer\n",
    "x = Conv2D(1024, (3,3), padding='same')(L_down)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "#Output layer\n",
    "x = Conv2D(1024, (3,3), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "\n",
    "#M1\n",
    "#Upsample F0\n",
    "x = UpSampling2D()(x)\n",
    "#Input layer\n",
    "L_down = AveragePooling2D(padding='same')(L)   #Downsample L\n",
    "n = int(math.log(sc/2,2) - 1)\n",
    "for i in range(n):\n",
    "    L_down = AveragePooling2D(padding='same')(L_down)    \n",
    "x = Concatenate()([L_down, x]) #Concatenate L_down and Fx_up\n",
    "#Intermediate layer\n",
    "x = Conv2D(1024, (3,3), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "#Output layer\n",
    "x = Conv2D(1024, (3,3), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "\n",
    "#M2\n",
    "#Upsample F1\n",
    "x = UpSampling2D()(x)\n",
    "#Input layer\n",
    "L_down = AveragePooling2D(padding='same')(L) #Downsample L\n",
    "n = int(math.log(sc/4,2) - 1)\n",
    "for i in range(n):\n",
    "    L_down = AveragePooling2D(padding='same')(L_down)\n",
    "x = Concatenate()([L_down, x]) #Concatenate L_down and Fx_up\n",
    "#Intermediate layer\n",
    "x = Conv2D(1024, (3,3), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "#Output layer\n",
    "x = Conv2D(1024, (3,3), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "\n",
    "#M3\n",
    "#Upsample F2\n",
    "x = UpSampling2D()(x)\n",
    "#Input layer\n",
    "L_down = AveragePooling2D(padding='same')(L) #Downsample L\n",
    "n = int(math.log(sc/8,2) - 1)\n",
    "for i in range(n):\n",
    "    L_down = AveragePooling2D(padding='same')(L_down)\n",
    "x = Concatenate()([L_down, x]) #Concatenate L_down and Fx_up\n",
    "#Intermediate layer\n",
    "x = Conv2D(512, (3,3), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "#Output layer\n",
    "x = Conv2D(512, (3,3), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "\n",
    "#M4\n",
    "#Upsample F3\n",
    "x = UpSampling2D()(x)\n",
    "#Input layer\n",
    "L_down = AveragePooling2D(padding='same')(L) #Downsample L\n",
    "n = int(math.log(sc/16,2) - 1)\n",
    "for i in range(n):\n",
    "    L_down = AveragePooling2D(padding='same')(L_down)\n",
    "x = Concatenate()([L_down, x]) #Concatenate L_down and Fx_up\n",
    "#Intermediate layer\n",
    "x = Conv2D(512, (3,3), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "#Output layer\n",
    "x = Conv2D(512, (3,3), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "\n",
    "#M5\n",
    "#Upsample F4\n",
    "x = UpSampling2D()(x)\n",
    "#Input layer\n",
    "L_down = AveragePooling2D(padding='same')(L) #Downsample L\n",
    "n = int(math.log(sc/32,2) - 1)\n",
    "for i in range(n):\n",
    "    L_down = AveragePooling2D(padding='same')(L_down)\n",
    "x = Concatenate()([L_down, x]) #Concatenate L_down and Fx_up\n",
    "#Intermediate layer\n",
    "x = Conv2D(128, (3,3), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "#Output layer\n",
    "x = Conv2D(128, (3,3), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "\n",
    "#M6\n",
    "#Upsample F5\n",
    "x = UpSampling2D()(x)\n",
    "#Input layer\n",
    "x = Concatenate()([L, x]) #Concatenate L and Fx_up\n",
    "#Intermediate layer\n",
    "x = Conv2D(32, (3,3), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "#Output layer - Apply 1x1 conv to F6\n",
    "x = Conv2D(3, (1,1), padding='same')(x)\n",
    "\n",
    "#Scale output so we get 0 to 255\n",
    "n1_out = Lambda(lambda x: (x+1)/2.0*255.0)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''NETWORK #2 - VGG19'''\n",
    "#We want to use VGG19 so that we know the perceptual difference between\n",
    "#reference image and generated image\n",
    "#Reference -> VGG19 -> reference image activation\n",
    "#Segmentation Map -> CRN -> VGG19 -> generated image activation\n",
    "\n",
    "#Note that there are 2 inputs for VGG network:\n",
    "#   1. Output of the generator network\n",
    "#   2. Reference image\n",
    "\n",
    "'''VGG input preprocessing as stated in the paper'''\n",
    "vgg_mean = np.array([123.68, 116.779, 103.939], dtype=np.float32)\n",
    "preprocess_vgg = lambda x: (x - vgg_mean)[:, :, :, ::-1]\n",
    "\n",
    "vgg_inp = Input(output_shape)\n",
    "vgg = VGG19(include_top=False,weights='imagenet',\n",
    "            input_tensor=Lambda(preprocess_vgg)(vgg_inp))\n",
    "\n",
    "#Make sure vgg layers are not trainable\n",
    "for layer in vgg.layers: \n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define model that will grab the activations from 5 conv layers\n",
    "vgg_content = Model(vgg_inp, [get_output(vgg, o) for o in [1,2,3,4,5]])\n",
    "\n",
    "#vgg1 = for the reference image\n",
    "vgg1 = vgg_content(vgg_inp)\n",
    "\n",
    "#vgg2 = for the generated image\n",
    "vgg2 = vgg_content(n1_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define weights: how much each conv layer output to effect the model\n",
    "layer_weights=[0.5, 0.25, 0.125, 0.0625, 0.0625]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define the model that actually minimizes the loss\n",
    "model = Model([L, vgg_inp], Lambda(content_fn)(vgg1+vgg2))\n",
    "\n",
    "#We want the output of our model (loss) to be zeros\n",
    "train_target = np.zeros((num_train, 1))\n",
    "val_target = np.zeros((num_validate, 1))\n",
    "\n",
    "model.compile(Adam(0.0001), loss='mean_squared_error', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer = Adam(0.0001)\n",
    "model.fit([x_train, y_train], train_target, \n",
    "          batch_size=3, epochs=250, \n",
    "          validation_data=([x_validate, y_validate],val_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = Model(L, n1_out)\n",
    "predictions = trained_model.predict(np.expand_dims(x_train[1], axis=0))\n",
    "plt.imshow(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
