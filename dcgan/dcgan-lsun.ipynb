{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D,\\\n",
    "    UpSampling2D, Lambda, Activation, Flatten, Reshape, merge\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.datasets import mnist\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from PIL import Image\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imsave\n",
    "import bcolz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test_, y_test_) = mnist.load_data()\n",
    "#x_train = bcolz.open('data/x_train.bc')[:]  #FOR LSUN DATASET\n",
    "x_train = (x_train.astype(np.float32)/255.0) #* 2.0 - 1.0\n",
    "x_train = np.expand_dims(x_train, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/activations.py:103: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  ).format(identifier=identifier.__class__.__name__))\n"
     ]
    }
   ],
   "source": [
    "discriminator_model = Sequential()\n",
    "discriminator_model.add(Conv2D(256, (5,5), strides=(2,2), padding='same', \n",
    "                  activation=LeakyReLU(), input_shape=(28, 28,1)))\n",
    "discriminator_model.add(Conv2D(512, (5,5), strides=(2,2), padding='same', \n",
    "                  activation=LeakyReLU()))\n",
    "discriminator_model.add(Flatten())\n",
    "discriminator_model.add(Dense(256, activation=LeakyReLU()))\n",
    "discriminator_model.add(Dense(1, activation = LeakyReLU()))\n",
    "discriminator_model.compile(Adam(), loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generator_model = Sequential()\n",
    "#generator_model.add(Dense(512*28*28, activation='relu', input_dim=100))\n",
    "generator_model.add(Dense(512*7*7, activation='relu', input_dim=100))\n",
    "generator_model.add(BatchNormalization())\n",
    "#generator_model.add(Reshape((28, 28, 512)))\n",
    "generator_model.add(Reshape((7, 7, 512)))\n",
    "generator_model.add(UpSampling2D())\n",
    "generator_model.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "generator_model.add(BatchNormalization())\n",
    "generator_model.add(UpSampling2D())\n",
    "generator_model.add(Conv2D(32, (3,3), padding='same', activation='relu'))\n",
    "generator_model.add(BatchNormalization())\n",
    "generator_model.add(Conv2D(1, (1,1), padding='same', activation='tanh'))\n",
    "#generator_model.add(Conv2D(3, (1,1), padding='same', activation='tanh')) #FOR RGB (LSUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfigure,axis = plt.subplots(2,3)\\nplt.tight_layout()\\nfor j in range(3):\\n    axis[0,j].imshow(real[j],cmap='gray')\\n    axis[0,j].set_yticklabels([])\\n    axis[0,j].set_xticklabels([])\\n    axis[1,j].imshow(fake[j],cmap='gray')\\n    axis[1,j].set_yticklabels([])\\n    axis[1,j].set_xticklabels([]\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Train discriminator for 1 epoch as a start'''\n",
    "init_data = len(x_train)//200\n",
    "\n",
    "real = np.random.permutation(x_train)[:init_data]\n",
    "fake = generator_model.predict(np.random.rand(init_data,100))\n",
    "x_init = np.concatenate((real,fake))\n",
    "\n",
    "real_label = np.zeros((init_data,1))\n",
    "fake_label = np.ones((init_data,1))\n",
    "y_init = np.concatenate((real_label,fake_label))\n",
    "\n",
    "'''\n",
    "figure,axis = plt.subplots(2,3)\n",
    "plt.tight_layout()\n",
    "for j in range(3):\n",
    "    axis[0,j].imshow(real[j],cmap='gray')\n",
    "    axis[0,j].set_yticklabels([])\n",
    "    axis[0,j].set_xticklabels([])\n",
    "    axis[1,j].imshow(fake[j],cmap='gray')\n",
    "    axis[1,j].set_yticklabels([])\n",
    "    axis[1,j].set_xticklabels([]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "600/600 [==============================] - 0s - loss: 8.0590     \n",
      "Epoch 2/5\n",
      "600/600 [==============================] - 0s - loss: 8.0590     \n",
      "Epoch 3/5\n",
      "600/600 [==============================] - 0s - loss: 8.0590     \n",
      "Epoch 4/5\n",
      "600/600 [==============================] - 0s - loss: 8.0590     \n",
      "Epoch 5/5\n",
      "600/600 [==============================] - 0s - loss: 8.0590     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4e28bd96d8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator_model.fit(x_init,y_init,batch_size=128,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([generator_model, discriminator_model])\n",
    "model.compile(Adam(), loss = \"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "real_per_iter = 50\n",
    "fake_per_iter = 50\n",
    "\n",
    "real_label = np.zeros((real_per_iter,1))\n",
    "fake_label = np.ones((fake_per_iter,1))\n",
    "y_mini = np.concatenate((real_label,fake_label))\n",
    "\n",
    "discriminator_loss = []\n",
    "generator_loss = []\n",
    "num_data = len(x_train)\n",
    "repeat_discriminator = 1\n",
    "repeat_generator = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_iterations = 300\n",
    "repeat_discriminator = 1\n",
    "repeat_generator = 1\n",
    "for i in range(num_iterations):\n",
    "    print('Iteration #%d' % \n",
    "          ((len(discriminator_loss)+(i*repeat_discriminator)-1)))\n",
    "    real = x_train[np.random.randint(0,high=num_data,size=real_per_iter)]\n",
    "    fake = generator_model.predict(np.random.rand(fake_per_iter,100))\n",
    "    x_mini = np.concatenate((real, fake))\n",
    "    '''Training discriminator'''\n",
    "    discriminator_model.fit(x_mini,y_mini,batch_size=25,epochs=1)\n",
    "    #for k in range(repeat_discriminator):\n",
    "        #discriminator_loss.append(discriminator_model.train_on_batch(x_mini,y_mini))\n",
    "    \n",
    "    '''Freeze discriminator'''\n",
    "    discriminator_model.trainable = False\n",
    "    for layer in discriminator_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    '''Training generator'''\n",
    "    generator_x = np.random.rand(fake_per_iter*2,100)\n",
    "    generator_y = np.zeros((fake_per_iter*2,1))\n",
    "    generator_model.fit(generator_x,generator_y,batch_size=25,epochs=1)\n",
    "    #for k in range(repeat_generator):\n",
    "        #generator_loss.append(model.train_on_batch(generator_x, generator_y))\n",
    "    \n",
    "    '''Unfreeze discriminator'''\n",
    "    discriminator_model.trainable = True\n",
    "    for layer in discriminator_model.layers:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    '''Show losses'''\n",
    "    #print('Discriminator loss = %.6f Generator loss = %.6f'\n",
    "          #% (discriminator_loss[len(discriminator_loss)-1],\n",
    "             #generator_loss[len(generator_loss)-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "figure,axis = plt.subplots(1,2)\n",
    "plt.tight_layout()\n",
    "axis[0].plot(discriminator_loss)\n",
    "axis[0].set_title(\"Discriminator Loss\")\n",
    "axis[1].plot(generator_loss)\n",
    "axis[1].set_title(\"Generator Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_generated = 9\n",
    "in_noise = np.random.rand(num_generated,100)\n",
    "im = generator_model.predict(in_noise)\n",
    "figure,axis = plt.subplots(3,3,figsize=(10,10))\n",
    "plt.tight_layout()\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        axis[i,j].imshow(im[i*3+j])\n",
    "        axis[i,j].set_yticklabels([])\n",
    "        axis[i,j].set_xticklabels([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
