{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D,\\\n",
    "    UpSampling2D, Dropout, Activation, Flatten, Reshape, merge, Conv2DTranspose\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.datasets import mnist\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.initializers import Initializer, RandomNormal\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from PIL import Image\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imsave\n",
    "import bcolz\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Note that in the paper, the output of the generative model\n",
    "is tanh activation layer, which will be the input to our\n",
    "discriminative model. So we need to normalize input images \n",
    "to tanh scale (-1.0 to 1.0)'''\n",
    "\n",
    "(x_train, y_train), (x_test_, y_test_) = mnist.load_data()\n",
    "x_train = (x_train.astype(np.float32)/255.0) * 2.0 - 1.0\n",
    "x_train = np.expand_dims(x_train, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Discriminator network'''\n",
    "discriminator_input = Input(x_train.shape[1:])\n",
    "discriminator_x = Conv2D(256, (5,5), strides=(2, 2), padding = 'same',\n",
    "          kernel_regularizer = l2(0.0001))(discriminator_input)\n",
    "discriminator_x = LeakyReLU(0.2)(discriminator_x)\n",
    "discriminator_x = Dropout(0.5)(discriminator_x)\n",
    "\n",
    "discriminator_x = Conv2D(512, (5,5), strides=(2, 2), padding = 'same',\n",
    "          kernel_regularizer = l2(0.0001))(discriminator_x)\n",
    "discriminator_x = LeakyReLU(0.2)(discriminator_x)\n",
    "discriminator_x = Dropout(0.5)(discriminator_x)\n",
    "discriminator_x = Flatten()(discriminator_x)\n",
    "\n",
    "discriminator_x = Dense(256, kernel_regularizer = l2(0.0001))(discriminator_x)\n",
    "discriminator_x = LeakyReLU(0.2)(discriminator_x)\n",
    "discriminator_x = Dropout(0.5)(discriminator_x)\n",
    "\n",
    "discriminator_x = Dense(1,activation='sigmoid')(discriminator_x)\n",
    "\n",
    "discriminator_model = Model(discriminator_input,discriminator_x)\n",
    "discriminator_model.compile(RMSprop(lr=0.00005),loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Generator network'''\n",
    "generator_input = Input((100,))\n",
    "generator_x = Dense(512*7*7, kernel_initializer=RandomNormal(\n",
    "                                   mean=0.0, stddev=0.02))(generator_input)\n",
    "generator_x = BatchNormalization()(generator_x)\n",
    "generator_x = Activation('relu')(generator_x)\n",
    "generator_x = Reshape((7,7,512))(generator_x)\n",
    "generator_x = UpSampling2D()(generator_x)\n",
    "\n",
    "generator_x = Conv2D(256, (3,3), padding='same', \n",
    "                  kernel_initializer=RandomNormal(\n",
    "                                   mean=0.0, stddev=0.02))(generator_x)\n",
    "generator_x = BatchNormalization()(generator_x)\n",
    "generator_x = Activation('relu')(generator_x)\n",
    "generator_x = UpSampling2D()(generator_x)\n",
    "\n",
    "generator_x = Conv2D(128, (3,3), padding='same',\n",
    "                  kernel_initializer=RandomNormal(\n",
    "                                   mean=0.0, stddev=0.02))(generator_x)\n",
    "generator_x = BatchNormalization()(generator_x)\n",
    "generator_x = Activation('relu')(generator_x)\n",
    "\n",
    "generator_x = Conv2D(64, (3,3), padding='same',\n",
    "                  kernel_initializer=RandomNormal(\n",
    "                                   mean=0.0, stddev=0.02))(generator_x)\n",
    "generator_x = BatchNormalization()(generator_x)\n",
    "generator_x = Activation('relu')(generator_x)\n",
    "\n",
    "generator_x = Conv2D(1, (1,1), padding='same',\n",
    "                  kernel_initializer=RandomNormal(\n",
    "                                   mean=0.0, stddev=0.02))(generator_x)\n",
    "generator_x = Activation('tanh')(generator_x)\n",
    "\n",
    "generator_model = Model(generator_input,generator_x)\n",
    "generator_model.compile(RMSprop(lr=0.00005),loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Freeze discriminator'''\n",
    "discriminator_model.trainable = False\n",
    "for layer in discriminator_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "'''GAN = generator + discriminator'''\n",
    "gan_input = Input((100,))\n",
    "gan_x = generator_model(gan_input)\n",
    "gan_x = discriminator_model(gan_x)\n",
    "model = Model(gan_input, gan_x)\n",
    "model.compile(RMSprop(lr=0.00005), loss = \"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Train discriminator for 1 epoch as a start'''\n",
    "init_data = len(x_train)//6\n",
    "\n",
    "real = np.random.permutation(x_train)[:init_data]\n",
    "fake = generator_model.predict(np.random.rand(init_data,100))\n",
    "x_init = np.concatenate((real,fake))\n",
    "\n",
    "real_label = np.zeros((init_data,1))\n",
    "fake_label = np.ones((init_data,1))\n",
    "y_init = np.concatenate((real_label,fake_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 5s - loss: 0.0840     \n"
     ]
    }
   ],
   "source": [
    "'''Unfreeze discriminator'''\n",
    "discriminator_model.trainable = True\n",
    "for layer in discriminator_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "discriminator_model.fit(x_init,y_init,batch_size=128,epochs=1)\n",
    "#Clamp the weights\n",
    "for layer in discriminator_model.layers:\n",
    "    weights = layer.get_weights()\n",
    "    weights = [np.clip(w, -0.01, 0.01) for w in weights]\n",
    "    layer.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discriminator_loss = []\n",
    "generator_loss = []\n",
    "num_data = len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #0\n",
      "Discriminator loss = 0.250558 Generator loss = 0.250875\n",
      "Iteration #1\n",
      "Discriminator loss = 0.251517 Generator loss = 0.250995\n",
      "Iteration #2\n",
      "Discriminator loss = 0.251219 Generator loss = 0.249265\n",
      "Iteration #3\n",
      "Discriminator loss = 0.249824 Generator loss = 0.251471\n",
      "Iteration #4\n",
      "Discriminator loss = 0.250932 Generator loss = 0.249792\n",
      "Iteration #5\n",
      "Discriminator loss = 0.251938 Generator loss = 0.253131\n",
      "Iteration #6\n",
      "Discriminator loss = 0.250284 Generator loss = 0.253649\n",
      "Iteration #7\n",
      "Discriminator loss = 0.250149 Generator loss = 0.255158\n",
      "Iteration #8\n",
      "Discriminator loss = 0.249793 Generator loss = 0.251296\n",
      "Iteration #9\n",
      "Discriminator loss = 0.250399 Generator loss = 0.250522\n",
      "Iteration #10\n",
      "Discriminator loss = 0.250312 Generator loss = 0.248285\n",
      "Iteration #11\n",
      "Discriminator loss = 0.250790 Generator loss = 0.254515\n",
      "Iteration #12\n",
      "Discriminator loss = 0.251023 Generator loss = 0.253227\n",
      "Iteration #13\n",
      "Discriminator loss = 0.250380 Generator loss = 0.253586\n",
      "Iteration #14\n",
      "Discriminator loss = 0.249839 Generator loss = 0.252105\n",
      "Iteration #15\n",
      "Discriminator loss = 0.249892 Generator loss = 0.256335\n",
      "Iteration #16\n",
      "Discriminator loss = 0.251802 Generator loss = 0.257551\n",
      "Iteration #17\n",
      "Discriminator loss = 0.250298 Generator loss = 0.256777\n",
      "Iteration #18\n",
      "Discriminator loss = 0.251463 Generator loss = 0.257089\n",
      "Iteration #19\n",
      "Discriminator loss = 0.250978 Generator loss = 0.253760\n",
      "Iteration #20\n",
      "Discriminator loss = 0.249960 Generator loss = 0.249117\n",
      "Iteration #21\n",
      "Discriminator loss = 0.249507 Generator loss = 0.245513\n",
      "Iteration #22\n",
      "Discriminator loss = 0.251337 Generator loss = 0.251800\n",
      "Iteration #23\n",
      "Discriminator loss = 0.248612 Generator loss = 0.252455\n",
      "Iteration #24\n",
      "Discriminator loss = 0.251763 Generator loss = 0.255597\n",
      "Iteration #25\n",
      "Discriminator loss = 0.247730 Generator loss = 0.248900\n",
      "Iteration #26\n",
      "Discriminator loss = 0.247976 Generator loss = 0.253180\n",
      "Iteration #27\n",
      "Discriminator loss = 0.252401 Generator loss = 0.255793\n",
      "Iteration #28\n",
      "Discriminator loss = 0.250383 Generator loss = 0.252636\n",
      "Iteration #29\n",
      "Discriminator loss = 0.251931 Generator loss = 0.251067\n",
      "Iteration #30\n",
      "Discriminator loss = 0.251605 Generator loss = 0.249432\n",
      "Iteration #31\n",
      "Discriminator loss = 0.251573 Generator loss = 0.245481\n",
      "Iteration #32\n",
      "Discriminator loss = 0.250532 Generator loss = 0.247238\n",
      "Iteration #33\n",
      "Discriminator loss = 0.251165 Generator loss = 0.250447\n",
      "Iteration #34\n",
      "Discriminator loss = 0.250172 Generator loss = 0.253298\n",
      "Iteration #35\n",
      "Discriminator loss = 0.250259 Generator loss = 0.254380\n",
      "Iteration #36\n",
      "Discriminator loss = 0.247931 Generator loss = 0.251799\n",
      "Iteration #37\n",
      "Discriminator loss = 0.250806 Generator loss = 0.248255\n",
      "Iteration #38\n",
      "Discriminator loss = 0.250506 Generator loss = 0.250551\n",
      "Iteration #39\n",
      "Discriminator loss = 0.250033 Generator loss = 0.248521\n",
      "Iteration #40\n",
      "Discriminator loss = 0.250680 Generator loss = 0.248264\n",
      "Iteration #41\n",
      "Discriminator loss = 0.251689 Generator loss = 0.249494\n",
      "Iteration #42\n",
      "Discriminator loss = 0.253592 Generator loss = 0.250807\n",
      "Iteration #43\n",
      "Discriminator loss = 0.249466 Generator loss = 0.248216\n",
      "Iteration #44\n",
      "Discriminator loss = 0.249577 Generator loss = 0.247672\n",
      "Iteration #45\n",
      "Discriminator loss = 0.252386 Generator loss = 0.246561\n",
      "Iteration #46\n",
      "Discriminator loss = 0.249536 Generator loss = 0.252837\n",
      "Iteration #47\n",
      "Discriminator loss = 0.249818 Generator loss = 0.251745\n",
      "Iteration #48\n",
      "Discriminator loss = 0.250371 Generator loss = 0.246530\n",
      "Iteration #49\n",
      "Discriminator loss = 0.251317 Generator loss = 0.253749\n",
      "Iteration #50\n",
      "Discriminator loss = 0.250600 Generator loss = 0.250739\n",
      "Iteration #51\n",
      "Discriminator loss = 0.247751 Generator loss = 0.256403\n",
      "Iteration #52\n",
      "Discriminator loss = 0.248954 Generator loss = 0.252411\n",
      "Iteration #53\n",
      "Discriminator loss = 0.247349 Generator loss = 0.251404\n",
      "Iteration #54\n",
      "Discriminator loss = 0.251174 Generator loss = 0.252007\n",
      "Iteration #55\n",
      "Discriminator loss = 0.250808 Generator loss = 0.255625\n",
      "Iteration #56\n",
      "Discriminator loss = 0.249461 Generator loss = 0.259687\n",
      "Iteration #57\n",
      "Discriminator loss = 0.248800 Generator loss = 0.261750\n",
      "Iteration #58\n",
      "Discriminator loss = 0.253051 Generator loss = 0.256468\n",
      "Iteration #59\n",
      "Discriminator loss = 0.252033 Generator loss = 0.255580\n",
      "Iteration #60\n",
      "Discriminator loss = 0.251097 Generator loss = 0.250543\n",
      "Iteration #61\n",
      "Discriminator loss = 0.251986 Generator loss = 0.247018\n",
      "Iteration #62\n",
      "Discriminator loss = 0.250197 Generator loss = 0.251368\n",
      "Iteration #63\n",
      "Discriminator loss = 0.250867 Generator loss = 0.256931\n",
      "Iteration #64\n",
      "Discriminator loss = 0.249699 Generator loss = 0.253307\n",
      "Iteration #65\n",
      "Discriminator loss = 0.252615 Generator loss = 0.255482\n",
      "Iteration #66\n",
      "Discriminator loss = 0.250444 Generator loss = 0.257927\n",
      "Iteration #67\n",
      "Discriminator loss = 0.251551 Generator loss = 0.255526\n",
      "Iteration #68\n",
      "Discriminator loss = 0.250839 Generator loss = 0.253402\n",
      "Iteration #69\n",
      "Discriminator loss = 0.248811 Generator loss = 0.256363\n",
      "Iteration #70\n",
      "Discriminator loss = 0.250333 Generator loss = 0.249738\n",
      "Iteration #71\n",
      "Discriminator loss = 0.250434 Generator loss = 0.250871\n",
      "Iteration #72\n",
      "Discriminator loss = 0.250715 Generator loss = 0.254644\n",
      "Iteration #73\n",
      "Discriminator loss = 0.252191 Generator loss = 0.249418\n",
      "Iteration #74\n",
      "Discriminator loss = 0.251271 Generator loss = 0.252117\n",
      "Iteration #75\n",
      "Discriminator loss = 0.252727 Generator loss = 0.247682\n",
      "Iteration #76\n",
      "Discriminator loss = 0.250197 Generator loss = 0.251338\n",
      "Iteration #77\n",
      "Discriminator loss = 0.250894 Generator loss = 0.250866\n",
      "Iteration #78\n",
      "Discriminator loss = 0.251985 Generator loss = 0.251203\n",
      "Iteration #79\n",
      "Discriminator loss = 0.250827 Generator loss = 0.252369\n",
      "Iteration #80\n",
      "Discriminator loss = 0.251256 Generator loss = 0.249926\n",
      "Iteration #81\n",
      "Discriminator loss = 0.249640 Generator loss = 0.252873\n",
      "Iteration #82\n",
      "Discriminator loss = 0.250721 Generator loss = 0.254544\n",
      "Iteration #83\n",
      "Discriminator loss = 0.251808 Generator loss = 0.251094\n",
      "Iteration #84\n",
      "Discriminator loss = 0.251580 Generator loss = 0.253340\n",
      "Iteration #85\n",
      "Discriminator loss = 0.252363 Generator loss = 0.247920\n",
      "Iteration #86\n",
      "Discriminator loss = 0.252152 Generator loss = 0.253227\n",
      "Iteration #87\n",
      "Discriminator loss = 0.251811 Generator loss = 0.251946\n",
      "Iteration #88\n",
      "Discriminator loss = 0.250344 Generator loss = 0.252569\n",
      "Iteration #89\n",
      "Discriminator loss = 0.250914 Generator loss = 0.252454\n",
      "Iteration #90\n",
      "Discriminator loss = 0.249909 Generator loss = 0.253982\n",
      "Iteration #91\n",
      "Discriminator loss = 0.249487 Generator loss = 0.253428\n",
      "Iteration #92\n",
      "Discriminator loss = 0.251038 Generator loss = 0.251689\n",
      "Iteration #93\n",
      "Discriminator loss = 0.249692 Generator loss = 0.258119\n",
      "Iteration #94\n",
      "Discriminator loss = 0.252570 Generator loss = 0.255851\n",
      "Iteration #95\n",
      "Discriminator loss = 0.249611 Generator loss = 0.251033\n",
      "Iteration #96\n",
      "Discriminator loss = 0.251998 Generator loss = 0.253403\n",
      "Iteration #97\n",
      "Discriminator loss = 0.251239 Generator loss = 0.252959\n",
      "Iteration #98\n",
      "Discriminator loss = 0.250324 Generator loss = 0.254209\n",
      "Iteration #99\n",
      "Discriminator loss = 0.249913 Generator loss = 0.254852\n",
      "Iteration #100\n",
      "Discriminator loss = 0.251862 Generator loss = 0.258747\n",
      "Iteration #101\n",
      "Discriminator loss = 0.249520 Generator loss = 0.251994\n",
      "Iteration #102\n",
      "Discriminator loss = 0.249814 Generator loss = 0.251527\n",
      "Iteration #103\n",
      "Discriminator loss = 0.252589 Generator loss = 0.251876\n",
      "Iteration #104\n",
      "Discriminator loss = 0.251890 Generator loss = 0.248391\n",
      "Iteration #105\n",
      "Discriminator loss = 0.251043 Generator loss = 0.248249\n",
      "Iteration #106\n",
      "Discriminator loss = 0.250047 Generator loss = 0.249480\n",
      "Iteration #107\n",
      "Discriminator loss = 0.251712 Generator loss = 0.247679\n",
      "Iteration #108\n",
      "Discriminator loss = 0.250788 Generator loss = 0.250361\n",
      "Iteration #109\n",
      "Discriminator loss = 0.251887 Generator loss = 0.253726\n",
      "Iteration #110\n",
      "Discriminator loss = 0.250543 Generator loss = 0.248810\n",
      "Iteration #111\n",
      "Discriminator loss = 0.250837 Generator loss = 0.246915\n",
      "Iteration #112\n",
      "Discriminator loss = 0.250848 Generator loss = 0.250456\n",
      "Iteration #113\n",
      "Discriminator loss = 0.249864 Generator loss = 0.248463\n",
      "Iteration #114\n",
      "Discriminator loss = 0.252635 Generator loss = 0.249408\n",
      "Iteration #115\n",
      "Discriminator loss = 0.253207 Generator loss = 0.245830\n",
      "Iteration #116\n",
      "Discriminator loss = 0.251354 Generator loss = 0.247643\n",
      "Iteration #117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss = 0.250653 Generator loss = 0.250366\n",
      "Iteration #118\n",
      "Discriminator loss = 0.249806 Generator loss = 0.252746\n",
      "Iteration #119\n",
      "Discriminator loss = 0.252857 Generator loss = 0.252397\n",
      "Iteration #120\n",
      "Discriminator loss = 0.252137 Generator loss = 0.251196\n",
      "Iteration #121\n",
      "Discriminator loss = 0.250853 Generator loss = 0.253280\n",
      "Iteration #122\n",
      "Discriminator loss = 0.250962 Generator loss = 0.252955\n",
      "Iteration #123\n",
      "Discriminator loss = 0.251094 Generator loss = 0.251276\n",
      "Iteration #124\n",
      "Discriminator loss = 0.250868 Generator loss = 0.250315\n",
      "Iteration #125\n",
      "Discriminator loss = 0.250832 Generator loss = 0.251381\n",
      "Iteration #126\n",
      "Discriminator loss = 0.252267 Generator loss = 0.252604\n",
      "Iteration #127\n",
      "Discriminator loss = 0.249689 Generator loss = 0.250603\n",
      "Iteration #128\n",
      "Discriminator loss = 0.251117 Generator loss = 0.252913\n",
      "Iteration #129\n",
      "Discriminator loss = 0.251710 Generator loss = 0.250910\n",
      "Iteration #130\n",
      "Discriminator loss = 0.248835 Generator loss = 0.252432\n",
      "Iteration #131\n",
      "Discriminator loss = 0.249255 Generator loss = 0.258725\n",
      "Iteration #132\n",
      "Discriminator loss = 0.250842 Generator loss = 0.252218\n",
      "Iteration #133\n",
      "Discriminator loss = 0.248516 Generator loss = 0.253093\n",
      "Iteration #134\n",
      "Discriminator loss = 0.250782 Generator loss = 0.247593\n",
      "Iteration #135\n",
      "Discriminator loss = 0.249923 Generator loss = 0.249215\n",
      "Iteration #136\n",
      "Discriminator loss = 0.251424 Generator loss = 0.253667\n",
      "Iteration #137\n",
      "Discriminator loss = 0.250859 Generator loss = 0.252961\n",
      "Iteration #138\n",
      "Discriminator loss = 0.249028 Generator loss = 0.255479\n",
      "Iteration #139\n",
      "Discriminator loss = 0.250329 Generator loss = 0.253748\n",
      "Iteration #140\n",
      "Discriminator loss = 0.251844 Generator loss = 0.254278\n",
      "Iteration #141\n",
      "Discriminator loss = 0.251616 Generator loss = 0.253216\n",
      "Iteration #142\n",
      "Discriminator loss = 0.251342 Generator loss = 0.249616\n",
      "Iteration #143\n",
      "Discriminator loss = 0.252097 Generator loss = 0.251831\n",
      "Iteration #144\n",
      "Discriminator loss = 0.251153 Generator loss = 0.247231\n",
      "Iteration #145\n",
      "Discriminator loss = 0.250569 Generator loss = 0.250352\n",
      "Iteration #146\n",
      "Discriminator loss = 0.251984 Generator loss = 0.254139\n",
      "Iteration #147\n",
      "Discriminator loss = 0.252591 Generator loss = 0.250821\n",
      "Iteration #148\n",
      "Discriminator loss = 0.249634 Generator loss = 0.254637\n",
      "Iteration #149\n",
      "Discriminator loss = 0.251937 Generator loss = 0.255296\n",
      "Iteration #150\n",
      "Discriminator loss = 0.249469 Generator loss = 0.251780\n",
      "Iteration #151\n",
      "Discriminator loss = 0.251338 Generator loss = 0.250605\n",
      "Iteration #152\n",
      "Discriminator loss = 0.250413 Generator loss = 0.249218\n",
      "Iteration #153\n",
      "Discriminator loss = 0.250403 Generator loss = 0.252327\n",
      "Iteration #154\n",
      "Discriminator loss = 0.249968 Generator loss = 0.250474\n",
      "Iteration #155\n",
      "Discriminator loss = 0.250910 Generator loss = 0.249557\n",
      "Iteration #156\n",
      "Discriminator loss = 0.251690 Generator loss = 0.256043\n",
      "Iteration #157\n",
      "Discriminator loss = 0.250851 Generator loss = 0.254827\n",
      "Iteration #158\n",
      "Discriminator loss = 0.252075 Generator loss = 0.252543\n",
      "Iteration #159\n",
      "Discriminator loss = 0.250872 Generator loss = 0.249479\n",
      "Iteration #160\n",
      "Discriminator loss = 0.251464 Generator loss = 0.249747\n",
      "Iteration #161\n",
      "Discriminator loss = 0.251118 Generator loss = 0.250862\n",
      "Iteration #162\n",
      "Discriminator loss = 0.252922 Generator loss = 0.253873\n",
      "Iteration #163\n",
      "Discriminator loss = 0.252021 Generator loss = 0.247999\n",
      "Iteration #164\n",
      "Discriminator loss = 0.252198 Generator loss = 0.250504\n",
      "Iteration #165\n",
      "Discriminator loss = 0.252790 Generator loss = 0.244471\n",
      "Iteration #166\n",
      "Discriminator loss = 0.250572 Generator loss = 0.246064\n",
      "Iteration #167\n",
      "Discriminator loss = 0.250624 Generator loss = 0.248112\n",
      "Iteration #168\n",
      "Discriminator loss = 0.251315 Generator loss = 0.249614\n",
      "Iteration #169\n",
      "Discriminator loss = 0.251016 Generator loss = 0.250006\n",
      "Iteration #170\n",
      "Discriminator loss = 0.251036 Generator loss = 0.248093\n",
      "Iteration #171\n",
      "Discriminator loss = 0.250058 Generator loss = 0.246088\n",
      "Iteration #172\n",
      "Discriminator loss = 0.249329 Generator loss = 0.247787\n",
      "Iteration #173\n",
      "Discriminator loss = 0.249161 Generator loss = 0.249547\n",
      "Iteration #174\n",
      "Discriminator loss = 0.250776 Generator loss = 0.245588\n",
      "Iteration #175\n",
      "Discriminator loss = 0.252334 Generator loss = 0.248966\n",
      "Iteration #176\n",
      "Discriminator loss = 0.251971 Generator loss = 0.249960\n",
      "Iteration #177\n",
      "Discriminator loss = 0.250698 Generator loss = 0.249052\n",
      "Iteration #178\n",
      "Discriminator loss = 0.250876 Generator loss = 0.250892\n",
      "Iteration #179\n",
      "Discriminator loss = 0.251978 Generator loss = 0.249961\n",
      "Iteration #180\n",
      "Discriminator loss = 0.252093 Generator loss = 0.245939\n",
      "Iteration #181\n",
      "Discriminator loss = 0.251588 Generator loss = 0.249390\n",
      "Iteration #182\n",
      "Discriminator loss = 0.250000 Generator loss = 0.252145\n",
      "Iteration #183\n",
      "Discriminator loss = 0.251067 Generator loss = 0.251969\n",
      "Iteration #184\n",
      "Discriminator loss = 0.252493 Generator loss = 0.248181\n",
      "Iteration #185\n",
      "Discriminator loss = 0.250482 Generator loss = 0.246288\n",
      "Iteration #186\n",
      "Discriminator loss = 0.250051 Generator loss = 0.247212\n",
      "Iteration #187\n",
      "Discriminator loss = 0.251090 Generator loss = 0.251711\n",
      "Iteration #188\n",
      "Discriminator loss = 0.250379 Generator loss = 0.254542\n",
      "Iteration #189\n",
      "Discriminator loss = 0.248037 Generator loss = 0.256922\n",
      "Iteration #190\n",
      "Discriminator loss = 0.249654 Generator loss = 0.256284\n",
      "Iteration #191\n",
      "Discriminator loss = 0.252220 Generator loss = 0.249238\n",
      "Iteration #192\n",
      "Discriminator loss = 0.250957 Generator loss = 0.252728\n",
      "Iteration #193\n",
      "Discriminator loss = 0.252833 Generator loss = 0.253466\n",
      "Iteration #194\n",
      "Discriminator loss = 0.251395 Generator loss = 0.250328\n",
      "Iteration #195\n",
      "Discriminator loss = 0.253152 Generator loss = 0.250620\n",
      "Iteration #196\n",
      "Discriminator loss = 0.252173 Generator loss = 0.251930\n",
      "Iteration #197\n",
      "Discriminator loss = 0.250555 Generator loss = 0.251974\n",
      "Iteration #198\n",
      "Discriminator loss = 0.250109 Generator loss = 0.251723\n",
      "Iteration #199\n",
      "Discriminator loss = 0.252467 Generator loss = 0.252298\n",
      "Iteration #200\n",
      "Discriminator loss = 0.251312 Generator loss = 0.256931\n",
      "Iteration #201\n",
      "Discriminator loss = 0.250395 Generator loss = 0.252228\n",
      "Iteration #202\n",
      "Discriminator loss = 0.251099 Generator loss = 0.250500\n",
      "Iteration #203\n",
      "Discriminator loss = 0.251725 Generator loss = 0.252331\n",
      "Iteration #204\n",
      "Discriminator loss = 0.252476 Generator loss = 0.252358\n",
      "Iteration #205\n",
      "Discriminator loss = 0.251044 Generator loss = 0.250145\n",
      "Iteration #206\n",
      "Discriminator loss = 0.250923 Generator loss = 0.251263\n",
      "Iteration #207\n",
      "Discriminator loss = 0.252558 Generator loss = 0.252713\n",
      "Iteration #208\n",
      "Discriminator loss = 0.250549 Generator loss = 0.251618\n",
      "Iteration #209\n",
      "Discriminator loss = 0.249910 Generator loss = 0.251265\n",
      "Iteration #210\n",
      "Discriminator loss = 0.251281 Generator loss = 0.251353\n",
      "Iteration #211\n",
      "Discriminator loss = 0.252069 Generator loss = 0.253338\n",
      "Iteration #212\n",
      "Discriminator loss = 0.249585 Generator loss = 0.251408\n",
      "Iteration #213\n",
      "Discriminator loss = 0.251887 Generator loss = 0.248961\n",
      "Iteration #214\n",
      "Discriminator loss = 0.252418 Generator loss = 0.251967\n",
      "Iteration #215\n",
      "Discriminator loss = 0.250032 Generator loss = 0.256751\n",
      "Iteration #216\n",
      "Discriminator loss = 0.250119 Generator loss = 0.250658\n",
      "Iteration #217\n",
      "Discriminator loss = 0.250260 Generator loss = 0.249778\n",
      "Iteration #218\n",
      "Discriminator loss = 0.252108 Generator loss = 0.253679\n",
      "Iteration #219\n",
      "Discriminator loss = 0.250916 Generator loss = 0.249478\n",
      "Iteration #220\n",
      "Discriminator loss = 0.251057 Generator loss = 0.248540\n",
      "Iteration #221\n",
      "Discriminator loss = 0.248987 Generator loss = 0.250381\n",
      "Iteration #222\n",
      "Discriminator loss = 0.251205 Generator loss = 0.254027\n",
      "Iteration #223\n",
      "Discriminator loss = 0.251793 Generator loss = 0.253955\n",
      "Iteration #224\n",
      "Discriminator loss = 0.250639 Generator loss = 0.254719\n",
      "Iteration #225\n",
      "Discriminator loss = 0.250126 Generator loss = 0.252872\n",
      "Iteration #226\n",
      "Discriminator loss = 0.253002 Generator loss = 0.253632\n",
      "Iteration #227\n",
      "Discriminator loss = 0.249998 Generator loss = 0.251730\n",
      "Iteration #228\n",
      "Discriminator loss = 0.251543 Generator loss = 0.252320\n",
      "Iteration #229\n",
      "Discriminator loss = 0.251292 Generator loss = 0.250843\n",
      "Iteration #230\n",
      "Discriminator loss = 0.249910 Generator loss = 0.251971\n",
      "Iteration #231\n",
      "Discriminator loss = 0.252422 Generator loss = 0.252800\n",
      "Iteration #232\n",
      "Discriminator loss = 0.249378 Generator loss = 0.250749\n",
      "Iteration #233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss = 0.251224 Generator loss = 0.251722\n",
      "Iteration #234\n",
      "Discriminator loss = 0.252206 Generator loss = 0.251192\n",
      "Iteration #235\n",
      "Discriminator loss = 0.251852 Generator loss = 0.252931\n",
      "Iteration #236\n",
      "Discriminator loss = 0.251658 Generator loss = 0.250283\n",
      "Iteration #237\n",
      "Discriminator loss = 0.251517 Generator loss = 0.249254\n",
      "Iteration #238\n",
      "Discriminator loss = 0.252098 Generator loss = 0.251145\n",
      "Iteration #239\n",
      "Discriminator loss = 0.250632 Generator loss = 0.247699\n",
      "Iteration #240\n",
      "Discriminator loss = 0.251655 Generator loss = 0.247155\n",
      "Iteration #241\n",
      "Discriminator loss = 0.250684 Generator loss = 0.248614\n",
      "Iteration #242\n",
      "Discriminator loss = 0.250877 Generator loss = 0.248503\n",
      "Iteration #243\n",
      "Discriminator loss = 0.250391 Generator loss = 0.250970\n",
      "Iteration #244\n",
      "Discriminator loss = 0.250170 Generator loss = 0.248162\n",
      "Iteration #245\n",
      "Discriminator loss = 0.251453 Generator loss = 0.252953\n",
      "Iteration #246\n",
      "Discriminator loss = 0.250797 Generator loss = 0.247667\n",
      "Iteration #247\n",
      "Discriminator loss = 0.251160 Generator loss = 0.250885\n",
      "Iteration #248\n",
      "Discriminator loss = 0.249963 Generator loss = 0.253042\n",
      "Iteration #249\n",
      "Discriminator loss = 0.249952 Generator loss = 0.251713\n",
      "Iteration #250\n",
      "Discriminator loss = 0.251629 Generator loss = 0.252728\n",
      "Iteration #251\n",
      "Discriminator loss = 0.249884 Generator loss = 0.255548\n",
      "Iteration #252\n",
      "Discriminator loss = 0.249486 Generator loss = 0.249395\n",
      "Iteration #253\n",
      "Discriminator loss = 0.249411 Generator loss = 0.251830\n",
      "Iteration #254\n",
      "Discriminator loss = 0.251179 Generator loss = 0.252355\n",
      "Iteration #255\n",
      "Discriminator loss = 0.250796 Generator loss = 0.253958\n",
      "Iteration #256\n",
      "Discriminator loss = 0.249426 Generator loss = 0.251798\n",
      "Iteration #257\n",
      "Discriminator loss = 0.249424 Generator loss = 0.251313\n",
      "Iteration #258\n",
      "Discriminator loss = 0.251590 Generator loss = 0.251527\n",
      "Iteration #259\n",
      "Discriminator loss = 0.250109 Generator loss = 0.253902\n",
      "Iteration #260\n",
      "Discriminator loss = 0.251389 Generator loss = 0.252923\n",
      "Iteration #261\n",
      "Discriminator loss = 0.249458 Generator loss = 0.249154\n",
      "Iteration #262\n",
      "Discriminator loss = 0.249601 Generator loss = 0.251208\n",
      "Iteration #263\n",
      "Discriminator loss = 0.249641 Generator loss = 0.248861\n",
      "Iteration #264\n",
      "Discriminator loss = 0.249481 Generator loss = 0.249056\n",
      "Iteration #265\n",
      "Discriminator loss = 0.252840 Generator loss = 0.250007\n",
      "Iteration #266\n",
      "Discriminator loss = 0.249958 Generator loss = 0.249604\n",
      "Iteration #267\n",
      "Discriminator loss = 0.250881 Generator loss = 0.251347\n",
      "Iteration #268\n",
      "Discriminator loss = 0.248969 Generator loss = 0.252556\n",
      "Iteration #269\n",
      "Discriminator loss = 0.250486 Generator loss = 0.249355\n",
      "Iteration #270\n",
      "Discriminator loss = 0.251561 Generator loss = 0.249868\n",
      "Iteration #271\n",
      "Discriminator loss = 0.248869 Generator loss = 0.250402\n",
      "Iteration #272\n",
      "Discriminator loss = 0.250555 Generator loss = 0.252421\n",
      "Iteration #273\n",
      "Discriminator loss = 0.252454 Generator loss = 0.251402\n",
      "Iteration #274\n",
      "Discriminator loss = 0.250432 Generator loss = 0.252144\n",
      "Iteration #275\n",
      "Discriminator loss = 0.252800 Generator loss = 0.250683\n",
      "Iteration #276\n",
      "Discriminator loss = 0.250829 Generator loss = 0.249218\n",
      "Iteration #277\n",
      "Discriminator loss = 0.250901 Generator loss = 0.250906\n",
      "Iteration #278\n",
      "Discriminator loss = 0.250399 Generator loss = 0.252209\n",
      "Iteration #279\n",
      "Discriminator loss = 0.254277 Generator loss = 0.251238\n",
      "Iteration #280\n",
      "Discriminator loss = 0.251004 Generator loss = 0.253922\n",
      "Iteration #281\n",
      "Discriminator loss = 0.250112 Generator loss = 0.251893\n",
      "Iteration #282\n",
      "Discriminator loss = 0.250232 Generator loss = 0.249221\n",
      "Iteration #283\n",
      "Discriminator loss = 0.252077 Generator loss = 0.252011\n",
      "Iteration #284\n",
      "Discriminator loss = 0.250979 Generator loss = 0.251726\n",
      "Iteration #285\n",
      "Discriminator loss = 0.251778 Generator loss = 0.249080\n",
      "Iteration #286\n",
      "Discriminator loss = 0.250515 Generator loss = 0.249428\n",
      "Iteration #287\n",
      "Discriminator loss = 0.250688 Generator loss = 0.251621\n",
      "Iteration #288\n",
      "Discriminator loss = 0.250871 Generator loss = 0.251670\n",
      "Iteration #289\n",
      "Discriminator loss = 0.250384 Generator loss = 0.250313\n",
      "Iteration #290\n",
      "Discriminator loss = 0.249967 Generator loss = 0.252885\n",
      "Iteration #291\n",
      "Discriminator loss = 0.251036 Generator loss = 0.248593\n",
      "Iteration #292\n",
      "Discriminator loss = 0.251540 Generator loss = 0.249178\n",
      "Iteration #293\n",
      "Discriminator loss = 0.249864 Generator loss = 0.250767\n",
      "Iteration #294\n",
      "Discriminator loss = 0.251584 Generator loss = 0.251556\n",
      "Iteration #295\n",
      "Discriminator loss = 0.250171 Generator loss = 0.252202\n",
      "Iteration #296\n",
      "Discriminator loss = 0.250140 Generator loss = 0.251804\n",
      "Iteration #297\n",
      "Discriminator loss = 0.250863 Generator loss = 0.254715\n",
      "Iteration #298\n",
      "Discriminator loss = 0.252805 Generator loss = 0.254743\n",
      "Iteration #299\n",
      "Discriminator loss = 0.249429 Generator loss = 0.250485\n",
      "Iteration #300\n",
      "Discriminator loss = 0.250609 Generator loss = 0.253838\n",
      "Iteration #301\n",
      "Discriminator loss = 0.250899 Generator loss = 0.249540\n",
      "Iteration #302\n",
      "Discriminator loss = 0.251002 Generator loss = 0.251584\n",
      "Iteration #303\n",
      "Discriminator loss = 0.250153 Generator loss = 0.256320\n",
      "Iteration #304\n",
      "Discriminator loss = 0.249935 Generator loss = 0.258752\n",
      "Iteration #305\n",
      "Discriminator loss = 0.253717 Generator loss = 0.254305\n",
      "Iteration #306\n",
      "Discriminator loss = 0.249480 Generator loss = 0.254811\n",
      "Iteration #307\n",
      "Discriminator loss = 0.250714 Generator loss = 0.254197\n",
      "Iteration #308\n",
      "Discriminator loss = 0.247647 Generator loss = 0.253986\n",
      "Iteration #309\n",
      "Discriminator loss = 0.251546 Generator loss = 0.250498\n",
      "Iteration #310\n",
      "Discriminator loss = 0.250870 Generator loss = 0.250398\n",
      "Iteration #311\n",
      "Discriminator loss = 0.250022 Generator loss = 0.252363\n",
      "Iteration #312\n",
      "Discriminator loss = 0.254355 Generator loss = 0.251706\n",
      "Iteration #313\n",
      "Discriminator loss = 0.253055 Generator loss = 0.249511\n",
      "Iteration #314\n",
      "Discriminator loss = 0.252905 Generator loss = 0.245849\n",
      "Iteration #315\n",
      "Discriminator loss = 0.251034 Generator loss = 0.248715\n",
      "Iteration #316\n",
      "Discriminator loss = 0.250914 Generator loss = 0.252841\n",
      "Iteration #317\n",
      "Discriminator loss = 0.249495 Generator loss = 0.254661\n",
      "Iteration #318\n",
      "Discriminator loss = 0.250768 Generator loss = 0.253704\n",
      "Iteration #319\n",
      "Discriminator loss = 0.249136 Generator loss = 0.252315\n",
      "Iteration #320\n",
      "Discriminator loss = 0.250349 Generator loss = 0.253922\n",
      "Iteration #321\n",
      "Discriminator loss = 0.249073 Generator loss = 0.251621\n",
      "Iteration #322\n",
      "Discriminator loss = 0.250056 Generator loss = 0.249900\n",
      "Iteration #323\n",
      "Discriminator loss = 0.249454 Generator loss = 0.251168\n",
      "Iteration #324\n",
      "Discriminator loss = 0.249864 Generator loss = 0.249556\n",
      "Iteration #325\n",
      "Discriminator loss = 0.250525 Generator loss = 0.249878\n",
      "Iteration #326\n",
      "Discriminator loss = 0.249029 Generator loss = 0.247730\n",
      "Iteration #327\n",
      "Discriminator loss = 0.252308 Generator loss = 0.253412\n",
      "Iteration #328\n",
      "Discriminator loss = 0.250107 Generator loss = 0.251539\n",
      "Iteration #329\n",
      "Discriminator loss = 0.252591 Generator loss = 0.252457\n",
      "Iteration #330\n",
      "Discriminator loss = 0.249727 Generator loss = 0.250260\n",
      "Iteration #331\n",
      "Discriminator loss = 0.252875 Generator loss = 0.250772\n",
      "Iteration #332\n",
      "Discriminator loss = 0.249075 Generator loss = 0.248762\n",
      "Iteration #333\n",
      "Discriminator loss = 0.250850 Generator loss = 0.253883\n",
      "Iteration #334\n",
      "Discriminator loss = 0.250765 Generator loss = 0.250959\n",
      "Iteration #335\n",
      "Discriminator loss = 0.249347 Generator loss = 0.253138\n",
      "Iteration #336\n",
      "Discriminator loss = 0.249662 Generator loss = 0.249999\n",
      "Iteration #337\n",
      "Discriminator loss = 0.250381 Generator loss = 0.252302\n",
      "Iteration #338\n",
      "Discriminator loss = 0.250241 Generator loss = 0.250760\n",
      "Iteration #339\n",
      "Discriminator loss = 0.249544 Generator loss = 0.251234\n",
      "Iteration #340\n",
      "Discriminator loss = 0.251117 Generator loss = 0.251204\n",
      "Iteration #341\n",
      "Discriminator loss = 0.251257 Generator loss = 0.249363\n",
      "Iteration #342\n",
      "Discriminator loss = 0.249625 Generator loss = 0.246226\n",
      "Iteration #343\n",
      "Discriminator loss = 0.249816 Generator loss = 0.249012\n",
      "Iteration #344\n",
      "Discriminator loss = 0.251016 Generator loss = 0.249642\n",
      "Iteration #345\n",
      "Discriminator loss = 0.251009 Generator loss = 0.248954\n",
      "Iteration #346\n",
      "Discriminator loss = 0.248803 Generator loss = 0.249557\n",
      "Iteration #347\n",
      "Discriminator loss = 0.250178 Generator loss = 0.248969\n",
      "Iteration #348\n",
      "Discriminator loss = 0.249902 Generator loss = 0.247269\n",
      "Iteration #349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss = 0.251296 Generator loss = 0.247218\n",
      "Iteration #350\n",
      "Discriminator loss = 0.251336 Generator loss = 0.248782\n",
      "Iteration #351\n",
      "Discriminator loss = 0.250765 Generator loss = 0.250885\n",
      "Iteration #352\n",
      "Discriminator loss = 0.250771 Generator loss = 0.250918\n",
      "Iteration #353\n",
      "Discriminator loss = 0.249965 Generator loss = 0.248873\n",
      "Iteration #354\n",
      "Discriminator loss = 0.249841 Generator loss = 0.251673\n",
      "Iteration #355\n",
      "Discriminator loss = 0.249786 Generator loss = 0.252221\n",
      "Iteration #356\n",
      "Discriminator loss = 0.249904 Generator loss = 0.250356\n",
      "Iteration #357\n",
      "Discriminator loss = 0.251295 Generator loss = 0.252040\n",
      "Iteration #358\n",
      "Discriminator loss = 0.251477 Generator loss = 0.250936\n",
      "Iteration #359\n",
      "Discriminator loss = 0.251059 Generator loss = 0.252850\n",
      "Iteration #360\n",
      "Discriminator loss = 0.251382 Generator loss = 0.249845\n",
      "Iteration #361\n",
      "Discriminator loss = 0.251295 Generator loss = 0.249538\n",
      "Iteration #362\n",
      "Discriminator loss = 0.251041 Generator loss = 0.251294\n",
      "Iteration #363\n",
      "Discriminator loss = 0.250791 Generator loss = 0.250267\n",
      "Iteration #364\n",
      "Discriminator loss = 0.251119 Generator loss = 0.251389\n",
      "Iteration #365\n",
      "Discriminator loss = 0.250580 Generator loss = 0.252833\n",
      "Iteration #366\n",
      "Discriminator loss = 0.250283 Generator loss = 0.251818\n",
      "Iteration #367\n",
      "Discriminator loss = 0.250673 Generator loss = 0.254134\n",
      "Iteration #368\n",
      "Discriminator loss = 0.250692 Generator loss = 0.254068\n",
      "Iteration #369\n",
      "Discriminator loss = 0.250679 Generator loss = 0.250064\n",
      "Iteration #370\n",
      "Discriminator loss = 0.250186 Generator loss = 0.250498\n",
      "Iteration #371\n",
      "Discriminator loss = 0.251542 Generator loss = 0.249649\n",
      "Iteration #372\n",
      "Discriminator loss = 0.250756 Generator loss = 0.252101\n",
      "Iteration #373\n",
      "Discriminator loss = 0.250398 Generator loss = 0.249457\n",
      "Iteration #374\n",
      "Discriminator loss = 0.250607 Generator loss = 0.252057\n",
      "Iteration #375\n",
      "Discriminator loss = 0.250280 Generator loss = 0.253373\n",
      "Iteration #376\n",
      "Discriminator loss = 0.250939 Generator loss = 0.252262\n",
      "Iteration #377\n",
      "Discriminator loss = 0.252179 Generator loss = 0.252952\n",
      "Iteration #378\n",
      "Discriminator loss = 0.251375 Generator loss = 0.250728\n",
      "Iteration #379\n",
      "Discriminator loss = 0.251235 Generator loss = 0.250143\n",
      "Iteration #380\n",
      "Discriminator loss = 0.249167 Generator loss = 0.251086\n",
      "Iteration #381\n",
      "Discriminator loss = 0.250714 Generator loss = 0.249635\n",
      "Iteration #382\n",
      "Discriminator loss = 0.250512 Generator loss = 0.249473\n",
      "Iteration #383\n",
      "Discriminator loss = 0.250994 Generator loss = 0.250362\n",
      "Iteration #384\n",
      "Discriminator loss = 0.250016 Generator loss = 0.252594\n",
      "Iteration #385\n",
      "Discriminator loss = 0.250315 Generator loss = 0.254024\n",
      "Iteration #386\n",
      "Discriminator loss = 0.249557 Generator loss = 0.252031\n",
      "Iteration #387\n",
      "Discriminator loss = 0.248881 Generator loss = 0.249067\n",
      "Iteration #388\n",
      "Discriminator loss = 0.250185 Generator loss = 0.249759\n",
      "Iteration #389\n",
      "Discriminator loss = 0.250519 Generator loss = 0.254230\n",
      "Iteration #390\n",
      "Discriminator loss = 0.250106 Generator loss = 0.249994\n",
      "Iteration #391\n",
      "Discriminator loss = 0.252131 Generator loss = 0.253284\n",
      "Iteration #392\n",
      "Discriminator loss = 0.249934 Generator loss = 0.251215\n",
      "Iteration #393\n",
      "Discriminator loss = 0.249756 Generator loss = 0.253246\n",
      "Iteration #394\n",
      "Discriminator loss = 0.247893 Generator loss = 0.250770\n",
      "Iteration #395\n",
      "Discriminator loss = 0.250859 Generator loss = 0.252373\n",
      "Iteration #396\n",
      "Discriminator loss = 0.248044 Generator loss = 0.252524\n",
      "Iteration #397\n",
      "Discriminator loss = 0.250997 Generator loss = 0.249337\n",
      "Iteration #398\n",
      "Discriminator loss = 0.251527 Generator loss = 0.248721\n",
      "Iteration #399\n",
      "Discriminator loss = 0.249734 Generator loss = 0.248248\n",
      "Iteration #400\n",
      "Discriminator loss = 0.249731 Generator loss = 0.251323\n",
      "Iteration #401\n",
      "Discriminator loss = 0.249462 Generator loss = 0.251913\n",
      "Iteration #402\n",
      "Discriminator loss = 0.251833 Generator loss = 0.254216\n",
      "Iteration #403\n",
      "Discriminator loss = 0.250537 Generator loss = 0.253890\n",
      "Iteration #404\n",
      "Discriminator loss = 0.250976 Generator loss = 0.252673\n",
      "Iteration #405\n",
      "Discriminator loss = 0.251449 Generator loss = 0.250622\n",
      "Iteration #406\n",
      "Discriminator loss = 0.250739 Generator loss = 0.251135\n",
      "Iteration #407\n",
      "Discriminator loss = 0.251704 Generator loss = 0.250885\n",
      "Iteration #408\n",
      "Discriminator loss = 0.250152 Generator loss = 0.248708\n",
      "Iteration #409\n",
      "Discriminator loss = 0.249883 Generator loss = 0.249124\n",
      "Iteration #410\n",
      "Discriminator loss = 0.250600 Generator loss = 0.251365\n",
      "Iteration #411\n",
      "Discriminator loss = 0.249670 Generator loss = 0.252563\n",
      "Iteration #412\n",
      "Discriminator loss = 0.251816 Generator loss = 0.252191\n",
      "Iteration #413\n",
      "Discriminator loss = 0.251253 Generator loss = 0.251690\n",
      "Iteration #414\n",
      "Discriminator loss = 0.251077 Generator loss = 0.250223\n",
      "Iteration #415\n",
      "Discriminator loss = 0.250760 Generator loss = 0.252481\n",
      "Iteration #416\n",
      "Discriminator loss = 0.251510 Generator loss = 0.250136\n",
      "Iteration #417\n",
      "Discriminator loss = 0.251268 Generator loss = 0.247321\n",
      "Iteration #418\n",
      "Discriminator loss = 0.250351 Generator loss = 0.248555\n",
      "Iteration #419\n",
      "Discriminator loss = 0.250361 Generator loss = 0.249413\n",
      "Iteration #420\n",
      "Discriminator loss = 0.250780 Generator loss = 0.247913\n",
      "Iteration #421\n",
      "Discriminator loss = 0.250673 Generator loss = 0.251629\n",
      "Iteration #422\n",
      "Discriminator loss = 0.249969 Generator loss = 0.250877\n",
      "Iteration #423\n",
      "Discriminator loss = 0.251252 Generator loss = 0.250620\n",
      "Iteration #424\n",
      "Discriminator loss = 0.251132 Generator loss = 0.252686\n",
      "Iteration #425\n",
      "Discriminator loss = 0.250797 Generator loss = 0.254829\n",
      "Iteration #426\n",
      "Discriminator loss = 0.249898 Generator loss = 0.253391\n",
      "Iteration #427\n",
      "Discriminator loss = 0.251434 Generator loss = 0.252138\n",
      "Iteration #428\n",
      "Discriminator loss = 0.251253 Generator loss = 0.252338\n",
      "Iteration #429\n",
      "Discriminator loss = 0.250835 Generator loss = 0.252055\n",
      "Iteration #430\n",
      "Discriminator loss = 0.250724 Generator loss = 0.249329\n",
      "Iteration #431\n",
      "Discriminator loss = 0.250172 Generator loss = 0.249047\n",
      "Iteration #432\n",
      "Discriminator loss = 0.250641 Generator loss = 0.251372\n",
      "Iteration #433\n",
      "Discriminator loss = 0.251346 Generator loss = 0.252093\n",
      "Iteration #434\n",
      "Discriminator loss = 0.251097 Generator loss = 0.253203\n",
      "Iteration #435\n",
      "Discriminator loss = 0.250722 Generator loss = 0.253980\n",
      "Iteration #436\n",
      "Discriminator loss = 0.250383 Generator loss = 0.253112\n",
      "Iteration #437\n",
      "Discriminator loss = 0.250752 Generator loss = 0.251898\n",
      "Iteration #438\n",
      "Discriminator loss = 0.251093 Generator loss = 0.250618\n",
      "Iteration #439\n",
      "Discriminator loss = 0.250033 Generator loss = 0.250484\n",
      "Iteration #440\n",
      "Discriminator loss = 0.250079 Generator loss = 0.249298\n",
      "Iteration #441\n",
      "Discriminator loss = 0.250325 Generator loss = 0.250546\n",
      "Iteration #442\n",
      "Discriminator loss = 0.250861 Generator loss = 0.250087\n",
      "Iteration #443\n",
      "Discriminator loss = 0.250325 Generator loss = 0.249065\n",
      "Iteration #444\n",
      "Discriminator loss = 0.251515 Generator loss = 0.250915\n",
      "Iteration #445\n",
      "Discriminator loss = 0.250934 Generator loss = 0.252067\n",
      "Iteration #446\n",
      "Discriminator loss = 0.250779 Generator loss = 0.253741\n",
      "Iteration #447\n",
      "Discriminator loss = 0.250477 Generator loss = 0.251841\n",
      "Iteration #448\n",
      "Discriminator loss = 0.250908 Generator loss = 0.249674\n",
      "Iteration #449\n",
      "Discriminator loss = 0.250732 Generator loss = 0.251233\n",
      "Iteration #450\n",
      "Discriminator loss = 0.250545 Generator loss = 0.250500\n",
      "Iteration #451\n",
      "Discriminator loss = 0.249919 Generator loss = 0.252149\n",
      "Iteration #452\n",
      "Discriminator loss = 0.249438 Generator loss = 0.253616\n",
      "Iteration #453\n",
      "Discriminator loss = 0.249588 Generator loss = 0.253589\n",
      "Iteration #454\n",
      "Discriminator loss = 0.250072 Generator loss = 0.250388\n",
      "Iteration #455\n",
      "Discriminator loss = 0.250462 Generator loss = 0.249205\n",
      "Iteration #456\n",
      "Discriminator loss = 0.250676 Generator loss = 0.249890\n",
      "Iteration #457\n",
      "Discriminator loss = 0.250892 Generator loss = 0.249818\n",
      "Iteration #458\n",
      "Discriminator loss = 0.252068 Generator loss = 0.251555\n",
      "Iteration #459\n",
      "Discriminator loss = 0.250028 Generator loss = 0.252682\n",
      "Iteration #460\n",
      "Discriminator loss = 0.250487 Generator loss = 0.251975\n",
      "Iteration #461\n",
      "Discriminator loss = 0.250076 Generator loss = 0.252854\n",
      "Iteration #462\n",
      "Discriminator loss = 0.250188 Generator loss = 0.252860\n",
      "Iteration #463\n",
      "Discriminator loss = 0.250771 Generator loss = 0.251914\n",
      "Iteration #464\n",
      "Discriminator loss = 0.251197 Generator loss = 0.251154\n",
      "Iteration #465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss = 0.249556 Generator loss = 0.251784\n",
      "Iteration #466\n",
      "Discriminator loss = 0.251537 Generator loss = 0.251385\n",
      "Iteration #467\n",
      "Discriminator loss = 0.250833 Generator loss = 0.252907\n",
      "Iteration #468\n",
      "Discriminator loss = 0.250631 Generator loss = 0.252231\n",
      "Iteration #469\n",
      "Discriminator loss = 0.250267 Generator loss = 0.252649\n",
      "Iteration #470\n",
      "Discriminator loss = 0.251176 Generator loss = 0.253448\n",
      "Iteration #471\n",
      "Discriminator loss = 0.249530 Generator loss = 0.251486\n",
      "Iteration #472\n",
      "Discriminator loss = 0.250583 Generator loss = 0.251318\n",
      "Iteration #473\n",
      "Discriminator loss = 0.250893 Generator loss = 0.249350\n",
      "Iteration #474\n",
      "Discriminator loss = 0.250038 Generator loss = 0.250128\n",
      "Iteration #475\n",
      "Discriminator loss = 0.250124 Generator loss = 0.248421\n",
      "Iteration #476\n",
      "Discriminator loss = 0.250640 Generator loss = 0.250142\n",
      "Iteration #477\n",
      "Discriminator loss = 0.250387 Generator loss = 0.249621\n",
      "Iteration #478\n",
      "Discriminator loss = 0.249563 Generator loss = 0.249779\n",
      "Iteration #479\n",
      "Discriminator loss = 0.250737 Generator loss = 0.250056\n",
      "Iteration #480\n",
      "Discriminator loss = 0.250909 Generator loss = 0.249149\n",
      "Iteration #481\n",
      "Discriminator loss = 0.250900 Generator loss = 0.248464\n",
      "Iteration #482\n",
      "Discriminator loss = 0.250600 Generator loss = 0.248053\n",
      "Iteration #483\n",
      "Discriminator loss = 0.250272 Generator loss = 0.248496\n",
      "Iteration #484\n",
      "Discriminator loss = 0.250846 Generator loss = 0.249089\n",
      "Iteration #485\n",
      "Discriminator loss = 0.251119 Generator loss = 0.247276\n",
      "Iteration #486\n",
      "Discriminator loss = 0.251053 Generator loss = 0.250869\n",
      "Iteration #487\n",
      "Discriminator loss = 0.250798 Generator loss = 0.251347\n",
      "Iteration #488\n",
      "Discriminator loss = 0.250052 Generator loss = 0.249999\n",
      "Iteration #489\n",
      "Discriminator loss = 0.249611 Generator loss = 0.249100\n",
      "Iteration #490\n",
      "Discriminator loss = 0.249834 Generator loss = 0.250350\n",
      "Iteration #491\n",
      "Discriminator loss = 0.250986 Generator loss = 0.250547\n",
      "Iteration #492\n",
      "Discriminator loss = 0.250645 Generator loss = 0.250937\n",
      "Iteration #493\n",
      "Discriminator loss = 0.250956 Generator loss = 0.251836\n",
      "Iteration #494\n",
      "Discriminator loss = 0.250569 Generator loss = 0.252488\n",
      "Iteration #495\n",
      "Discriminator loss = 0.250356 Generator loss = 0.252508\n",
      "Iteration #496\n",
      "Discriminator loss = 0.250651 Generator loss = 0.254014\n",
      "Iteration #497\n",
      "Discriminator loss = 0.249920 Generator loss = 0.250073\n",
      "Iteration #498\n",
      "Discriminator loss = 0.251382 Generator loss = 0.253321\n",
      "Iteration #499\n",
      "Discriminator loss = 0.250104 Generator loss = 0.252318\n",
      "Iteration #500\n",
      "Discriminator loss = 0.251400 Generator loss = 0.251354\n",
      "Iteration #501\n",
      "Discriminator loss = 0.251144 Generator loss = 0.251404\n",
      "Iteration #502\n",
      "Discriminator loss = 0.249705 Generator loss = 0.250879\n",
      "Iteration #503\n",
      "Discriminator loss = 0.250055 Generator loss = 0.249373\n",
      "Iteration #504\n",
      "Discriminator loss = 0.250780 Generator loss = 0.248279\n",
      "Iteration #505\n",
      "Discriminator loss = 0.251356 Generator loss = 0.250204\n",
      "Iteration #506\n",
      "Discriminator loss = 0.250536 Generator loss = 0.250017\n",
      "Iteration #507\n",
      "Discriminator loss = 0.250710 Generator loss = 0.249920\n",
      "Iteration #508\n",
      "Discriminator loss = 0.251328 Generator loss = 0.251784\n",
      "Iteration #509\n",
      "Discriminator loss = 0.249482 Generator loss = 0.252311\n",
      "Iteration #510\n",
      "Discriminator loss = 0.250344 Generator loss = 0.252873\n",
      "Iteration #511\n",
      "Discriminator loss = 0.249851 Generator loss = 0.252523\n",
      "Iteration #512\n",
      "Discriminator loss = 0.250099 Generator loss = 0.253135\n",
      "Iteration #513\n",
      "Discriminator loss = 0.249438 Generator loss = 0.251490\n",
      "Iteration #514\n",
      "Discriminator loss = 0.250726 Generator loss = 0.250521\n",
      "Iteration #515\n",
      "Discriminator loss = 0.250444 Generator loss = 0.249646\n",
      "Iteration #516\n",
      "Discriminator loss = 0.251257 Generator loss = 0.252163\n",
      "Iteration #517\n",
      "Discriminator loss = 0.248632 Generator loss = 0.251407\n",
      "Iteration #518\n",
      "Discriminator loss = 0.251359 Generator loss = 0.250964\n",
      "Iteration #519\n",
      "Discriminator loss = 0.249520 Generator loss = 0.247753\n",
      "Iteration #520\n",
      "Discriminator loss = 0.251236 Generator loss = 0.247204\n",
      "Iteration #521\n",
      "Discriminator loss = 0.249881 Generator loss = 0.248074\n",
      "Iteration #522\n",
      "Discriminator loss = 0.250539 Generator loss = 0.247955\n",
      "Iteration #523\n",
      "Discriminator loss = 0.251205 Generator loss = 0.247484\n",
      "Iteration #524\n",
      "Discriminator loss = 0.250259 Generator loss = 0.250647\n",
      "Iteration #525\n",
      "Discriminator loss = 0.251032 Generator loss = 0.248942\n",
      "Iteration #526\n",
      "Discriminator loss = 0.249917 Generator loss = 0.250396\n",
      "Iteration #527\n",
      "Discriminator loss = 0.249654 Generator loss = 0.253459\n",
      "Iteration #528\n",
      "Discriminator loss = 0.250339 Generator loss = 0.254503\n",
      "Iteration #529\n",
      "Discriminator loss = 0.251594 Generator loss = 0.254383\n",
      "Iteration #530\n",
      "Discriminator loss = 0.249799 Generator loss = 0.252984\n",
      "Iteration #531\n",
      "Discriminator loss = 0.251060 Generator loss = 0.249941\n",
      "Iteration #532\n",
      "Discriminator loss = 0.251113 Generator loss = 0.251718\n",
      "Iteration #533\n",
      "Discriminator loss = 0.250201 Generator loss = 0.250636\n",
      "Iteration #534\n",
      "Discriminator loss = 0.251097 Generator loss = 0.250268\n",
      "Iteration #535\n",
      "Discriminator loss = 0.250660 Generator loss = 0.249071\n",
      "Iteration #536\n",
      "Discriminator loss = 0.249182 Generator loss = 0.250786\n",
      "Iteration #537\n",
      "Discriminator loss = 0.249985 Generator loss = 0.249976\n",
      "Iteration #538\n",
      "Discriminator loss = 0.249051 Generator loss = 0.250134\n",
      "Iteration #539\n",
      "Discriminator loss = 0.251284 Generator loss = 0.251294\n",
      "Iteration #540\n",
      "Discriminator loss = 0.250982 Generator loss = 0.248885\n",
      "Iteration #541\n",
      "Discriminator loss = 0.251755 Generator loss = 0.249907\n",
      "Iteration #542\n",
      "Discriminator loss = 0.250294 Generator loss = 0.250531\n",
      "Iteration #543\n",
      "Discriminator loss = 0.249887 Generator loss = 0.250310\n",
      "Iteration #544\n",
      "Discriminator loss = 0.250392 Generator loss = 0.251913\n",
      "Iteration #545\n",
      "Discriminator loss = 0.250799 Generator loss = 0.251844\n",
      "Iteration #546\n",
      "Discriminator loss = 0.249858 Generator loss = 0.252829\n",
      "Iteration #547\n",
      "Discriminator loss = 0.251575 Generator loss = 0.252559\n",
      "Iteration #548\n",
      "Discriminator loss = 0.251296 Generator loss = 0.248031\n",
      "Iteration #549\n",
      "Discriminator loss = 0.251306 Generator loss = 0.252100\n",
      "Iteration #550\n",
      "Discriminator loss = 0.251482 Generator loss = 0.252168\n",
      "Iteration #551\n",
      "Discriminator loss = 0.250820 Generator loss = 0.256518\n",
      "Iteration #552\n",
      "Discriminator loss = 0.252149 Generator loss = 0.251649\n",
      "Iteration #553\n",
      "Discriminator loss = 0.249055 Generator loss = 0.252264\n",
      "Iteration #554\n",
      "Discriminator loss = 0.250139 Generator loss = 0.253251\n",
      "Iteration #555\n",
      "Discriminator loss = 0.250696 Generator loss = 0.252043\n",
      "Iteration #556\n",
      "Discriminator loss = 0.250816 Generator loss = 0.251127\n",
      "Iteration #557\n",
      "Discriminator loss = 0.250241 Generator loss = 0.250046\n",
      "Iteration #558\n",
      "Discriminator loss = 0.250558 Generator loss = 0.250542\n",
      "Iteration #559\n",
      "Discriminator loss = 0.250187 Generator loss = 0.252204\n",
      "Iteration #560\n",
      "Discriminator loss = 0.251813 Generator loss = 0.251465\n",
      "Iteration #561\n",
      "Discriminator loss = 0.250997 Generator loss = 0.251531\n",
      "Iteration #562\n",
      "Discriminator loss = 0.250918 Generator loss = 0.250677\n",
      "Iteration #563\n",
      "Discriminator loss = 0.250083 Generator loss = 0.251254\n",
      "Iteration #564\n",
      "Discriminator loss = 0.250761 Generator loss = 0.252705\n",
      "Iteration #565\n",
      "Discriminator loss = 0.251638 Generator loss = 0.249167\n",
      "Iteration #566\n",
      "Discriminator loss = 0.250949 Generator loss = 0.249889\n",
      "Iteration #567\n",
      "Discriminator loss = 0.250412 Generator loss = 0.249117\n",
      "Iteration #568\n",
      "Discriminator loss = 0.250173 Generator loss = 0.247544\n",
      "Iteration #569\n",
      "Discriminator loss = 0.250998 Generator loss = 0.251173\n",
      "Iteration #570\n",
      "Discriminator loss = 0.251502 Generator loss = 0.252042\n",
      "Iteration #571\n",
      "Discriminator loss = 0.249961 Generator loss = 0.249945\n",
      "Iteration #572\n",
      "Discriminator loss = 0.250830 Generator loss = 0.250886\n",
      "Iteration #573\n",
      "Discriminator loss = 0.250207 Generator loss = 0.251368\n",
      "Iteration #574\n",
      "Discriminator loss = 0.250237 Generator loss = 0.253379\n",
      "Iteration #575\n",
      "Discriminator loss = 0.249328 Generator loss = 0.249934\n",
      "Iteration #576\n",
      "Discriminator loss = 0.252688 Generator loss = 0.249174\n",
      "Iteration #577\n",
      "Discriminator loss = 0.250598 Generator loss = 0.249623\n",
      "Iteration #578\n",
      "Discriminator loss = 0.250724 Generator loss = 0.248895\n",
      "Iteration #579\n",
      "Discriminator loss = 0.249719 Generator loss = 0.251996\n",
      "Iteration #580\n",
      "Discriminator loss = 0.250369 Generator loss = 0.253711\n",
      "Iteration #581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss = 0.250117 Generator loss = 0.255191\n",
      "Iteration #582\n",
      "Discriminator loss = 0.250649 Generator loss = 0.257392\n",
      "Iteration #583\n",
      "Discriminator loss = 0.250920 Generator loss = 0.256153\n",
      "Iteration #584\n",
      "Discriminator loss = 0.251870 Generator loss = 0.254514\n",
      "Iteration #585\n",
      "Discriminator loss = 0.250407 Generator loss = 0.253549\n",
      "Iteration #586\n",
      "Discriminator loss = 0.249787 Generator loss = 0.252760\n",
      "Iteration #587\n",
      "Discriminator loss = 0.250379 Generator loss = 0.250390\n",
      "Iteration #588\n",
      "Discriminator loss = 0.249677 Generator loss = 0.247966\n",
      "Iteration #589\n",
      "Discriminator loss = 0.249134 Generator loss = 0.246852\n",
      "Iteration #590\n",
      "Discriminator loss = 0.250675 Generator loss = 0.249274\n",
      "Iteration #591\n",
      "Discriminator loss = 0.251071 Generator loss = 0.250821\n",
      "Iteration #592\n",
      "Discriminator loss = 0.250803 Generator loss = 0.249011\n",
      "Iteration #593\n",
      "Discriminator loss = 0.250327 Generator loss = 0.252225\n",
      "Iteration #594\n",
      "Discriminator loss = 0.251350 Generator loss = 0.252476\n",
      "Iteration #595\n",
      "Discriminator loss = 0.250289 Generator loss = 0.250908\n",
      "Iteration #596\n",
      "Discriminator loss = 0.251227 Generator loss = 0.249380\n",
      "Iteration #597\n",
      "Discriminator loss = 0.250680 Generator loss = 0.251047\n",
      "Iteration #598\n",
      "Discriminator loss = 0.251074 Generator loss = 0.251300\n",
      "Iteration #599\n",
      "Discriminator loss = 0.251205 Generator loss = 0.250514\n",
      "Iteration #600\n",
      "Discriminator loss = 0.250569 Generator loss = 0.249097\n",
      "Iteration #601\n",
      "Discriminator loss = 0.249542 Generator loss = 0.249529\n",
      "Iteration #602\n",
      "Discriminator loss = 0.249957 Generator loss = 0.251036\n",
      "Iteration #603\n",
      "Discriminator loss = 0.251633 Generator loss = 0.251498\n",
      "Iteration #604\n",
      "Discriminator loss = 0.249776 Generator loss = 0.251463\n",
      "Iteration #605\n",
      "Discriminator loss = 0.250816 Generator loss = 0.251598\n",
      "Iteration #606\n",
      "Discriminator loss = 0.252108 Generator loss = 0.251992\n",
      "Iteration #607\n",
      "Discriminator loss = 0.250209 Generator loss = 0.251082\n",
      "Iteration #608\n",
      "Discriminator loss = 0.250791 Generator loss = 0.252215\n",
      "Iteration #609\n",
      "Discriminator loss = 0.251495 Generator loss = 0.251885\n",
      "Iteration #610\n",
      "Discriminator loss = 0.251084 Generator loss = 0.251014\n",
      "Iteration #611\n",
      "Discriminator loss = 0.251407 Generator loss = 0.251192\n",
      "Iteration #612\n",
      "Discriminator loss = 0.249883 Generator loss = 0.250740\n",
      "Iteration #613\n",
      "Discriminator loss = 0.251451 Generator loss = 0.250193\n",
      "Iteration #614\n",
      "Discriminator loss = 0.250642 Generator loss = 0.252275\n",
      "Iteration #615\n",
      "Discriminator loss = 0.249819 Generator loss = 0.250349\n",
      "Iteration #616\n",
      "Discriminator loss = 0.250598 Generator loss = 0.253179\n",
      "Iteration #617\n",
      "Discriminator loss = 0.250387 Generator loss = 0.250901\n",
      "Iteration #618\n",
      "Discriminator loss = 0.250843 Generator loss = 0.251254\n",
      "Iteration #619\n",
      "Discriminator loss = 0.250684 Generator loss = 0.249883\n",
      "Iteration #620\n",
      "Discriminator loss = 0.249819 Generator loss = 0.250932\n",
      "Iteration #621\n",
      "Discriminator loss = 0.249563 Generator loss = 0.251219\n",
      "Iteration #622\n",
      "Discriminator loss = 0.250137 Generator loss = 0.250230\n",
      "Iteration #623\n",
      "Discriminator loss = 0.250834 Generator loss = 0.250736\n",
      "Iteration #624\n",
      "Discriminator loss = 0.249703 Generator loss = 0.252678\n",
      "Iteration #625\n",
      "Discriminator loss = 0.250445 Generator loss = 0.250570\n",
      "Iteration #626\n",
      "Discriminator loss = 0.250373 Generator loss = 0.252688\n",
      "Iteration #627\n",
      "Discriminator loss = 0.248901 Generator loss = 0.252513\n",
      "Iteration #628\n",
      "Discriminator loss = 0.249903 Generator loss = 0.254305\n",
      "Iteration #629\n",
      "Discriminator loss = 0.251574 Generator loss = 0.252312\n",
      "Iteration #630\n",
      "Discriminator loss = 0.250751 Generator loss = 0.251734\n",
      "Iteration #631\n",
      "Discriminator loss = 0.250400 Generator loss = 0.252491\n",
      "Iteration #632\n",
      "Discriminator loss = 0.250385 Generator loss = 0.251172\n",
      "Iteration #633\n",
      "Discriminator loss = 0.249976 Generator loss = 0.252585\n",
      "Iteration #634\n",
      "Discriminator loss = 0.251353 Generator loss = 0.251355\n",
      "Iteration #635\n",
      "Discriminator loss = 0.250431 Generator loss = 0.251291\n",
      "Iteration #636\n",
      "Discriminator loss = 0.249326 Generator loss = 0.254350\n",
      "Iteration #637\n",
      "Discriminator loss = 0.250288 Generator loss = 0.252544\n",
      "Iteration #638\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-66ca4ab0314c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;34m'''Freeze discriminator'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1190\u001b[0m                                  'provided weight shape ' + str(w.shape))\n\u001b[1;32m   1191\u001b[0m             \u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m         \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2186\u001b[0m             \u001b[0massign_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2187\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2188\u001b[0;31m         \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''Note that our discriminator and generator models are referenced\n",
    "by our GAN and we do not want the discriminator part of the GAN to be\n",
    "trained when trying to minimize generator losses. To do so, we can just\n",
    "do it by freezing discriminator model before training the generator'''\n",
    "\n",
    "model.optimizer = RMSprop(lr=0.00005)\n",
    "discriminator_model.optimizer = RMSprop(lr=0.00005)\n",
    "generator_model.optimizer = RMSprop(lr=0.00005)\n",
    "\n",
    "num_iterations = 50000\n",
    "real_per_iter = 64\n",
    "fake_per_iter = 64\n",
    "\n",
    "real_label = np.zeros((real_per_iter,1))\n",
    "fake_label = np.ones((fake_per_iter,1))\n",
    "y_mini = np.concatenate((real_label,fake_label))\n",
    "\n",
    "repeat_discriminator = 100\n",
    "repeat_generator = 1\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    print('Iteration #%d' % i)\n",
    "    if (len(discriminator_loss)>25):\n",
    "        repeat_discriminator = 5\n",
    "    \n",
    "    '''Unfreeze discriminator'''\n",
    "    discriminator_model.trainable = True\n",
    "    for layer in discriminator_model.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    '''Training discriminator'''\n",
    "    for k in range(repeat_discriminator):\n",
    "        '''Create real and fake data, as well as their labels'''\n",
    "        real = x_train[np.random.randint(0,high=num_data,size=real_per_iter)]\n",
    "        fake = generator_model.predict(np.random.rand(fake_per_iter,100))\n",
    "        x_mini = np.concatenate((real, fake))\n",
    "        discriminator_loss.append(discriminator_model.train_on_batch(x_mini,y_mini))\n",
    "        #Clamp the weights\n",
    "        for layer in discriminator_model.layers:\n",
    "            weights = layer.get_weights()\n",
    "            weights = [np.clip(w, -0.01, 0.01) for w in weights]\n",
    "            layer.set_weights(weights)\n",
    "    \n",
    "    '''Freeze discriminator'''\n",
    "    discriminator_model.trainable = False\n",
    "    for layer in discriminator_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    '''Training generator'''\n",
    "    for k in range(repeat_generator):\n",
    "        generator_x = np.random.rand(fake_per_iter*2,100)\n",
    "        generator_y = np.zeros((fake_per_iter*2,1)) #we want it to be classified as real\n",
    "        generator_loss.append(model.train_on_batch(generator_x, generator_y))\n",
    "    \n",
    "    '''Show losses'''\n",
    "    print('Discriminator loss = %.6f Generator loss = %.6f'\n",
    "          % (discriminator_loss[len(discriminator_loss)-1],\n",
    "             generator_loss[len(generator_loss)-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f36b5311438>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaQAAAEkCAYAAABg/EXtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nApIxxphA+H+thd2tkaa7cgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f36b52c2668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Plot discriminator and generator losses'''\n",
    "figure,axis = plt.subplots(1,2)\n",
    "plt.tight_layout()\n",
    "axis[0].plot(discriminator_loss)\n",
    "axis[0].set_title(\"Discriminator Loss\")\n",
    "axis[1].plot(generator_loss)\n",
    "axis[1].set_title(\"Generator Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArAAAAKwCAYAAABgREy2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f36b54ea588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Generate images with our trained generator model'''\n",
    "num_generated = 9\n",
    "in_noise = np.random.rand(num_generated,100)\n",
    "im = generator_model.predict(in_noise)\n",
    "im = im[:,:,:,0]\n",
    "figure,axis = plt.subplots(3,3,figsize=(10,10))\n",
    "plt.tight_layout()\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        axis[i,j].imshow(im[i*3+j],cmap='gray')\n",
    "        axis[i,j].set_yticklabels([])\n",
    "        axis[i,j].set_xticklabels([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Save weights and models'''\n",
    "discriminator_model.save_weights('wasserstein_mnist_discriminator_deepmodel_weights.h5')\n",
    "generator_model.save_weights('wasserstein_mnist_generator_deepmodel_weights.h5')\n",
    "model.save_weights('wasserstein_mnist_gan_deepmodel_weights.h5')\n",
    "\n",
    "discriminator_model.save('wasserstein_mnist_discriminator_deepmodel.h5')\n",
    "generator_model.save('wasserstein_mnist_generator_deepmodel.h5')\n",
    "model.save('wasserstein_mnist_gan_deepmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
