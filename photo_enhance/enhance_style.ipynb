{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D,\\\n",
    "        UpSampling2D, Lambda, Activation, merge, add\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from PIL import Image\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imsave\n",
    "import bcolz\n",
    "import pydot\n",
    "import graphviz\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "x_train = np.load('data/x_train.npy')\n",
    "y_train = np.load('data/y_train.npy')\n",
    "x_validate = np.load('data/x_validate.npy')\n",
    "y_validate = np.load('data/y_validate.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_shape = y_train[0].shape\n",
    "num_images = y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolution_block(x, filters, size, strides=(1,1), padding='same', act=True):\n",
    "    x = Conv2D(filters, (size,size), strides=strides, padding=padding)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    if act == True:\n",
    "        x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def residual_block(blockInput, num_filters=64):\n",
    "    x = convolution_block(blockInput, num_filters, 3)\n",
    "    x = convolution_block(x, num_filters, 3, act=False)\n",
    "    x = merge([x, blockInput], mode='sum')\n",
    "    return x\n",
    "\n",
    "def upsampling_block(x, filters, size):\n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2D(filters, (size,size), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:11: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/usr/local/lib/python3.5/dist-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    }
   ],
   "source": [
    "'''NETWORK #1'''\n",
    "inp=Input(x_train.shape[1:])\n",
    "x=convolution_block(inp, 64, 9)\n",
    "x=residual_block(x)\n",
    "x=residual_block(x)\n",
    "x=residual_block(x)\n",
    "x=residual_block(x)\n",
    "x=Conv2D(3, (9,9), activation='tanh', padding='same')(x)\n",
    "n1_out=Lambda(lambda x: (x+1)*127.5)(x) #scale output so we get 0 to 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''NETWORK #2 - VGG'''\n",
    "#We want to use VGG so that we know the difference in activation between\n",
    "#high-res image and output of the low-res image\n",
    "#High-res -> VGG -> high-res activation\n",
    "#Low-res -> trainableCNN -> VGG -> generated image activation\n",
    "\n",
    "#Note that there are 2 inputs for VGG network:\n",
    "#   1. Output of the low-res image from trainable network\n",
    "#   2. High-res image\n",
    "\n",
    "'''VGG input preprocessing as stated in the paper'''\n",
    "vgg_mean = np.array([123.68, 116.779, 103.939], dtype=np.float32)\n",
    "preprocess_vgg = lambda x: (x - vgg_mean)[:, :, :, ::-1]\n",
    "\n",
    "vgg_inp=Input(output_shape)\n",
    "vgg= VGG16(include_top=False, input_tensor=Lambda(preprocess_vgg)(vgg_inp))\n",
    "\n",
    "for layer in vgg.layers: \n",
    "    layer.trainable=False\n",
    "\n",
    "#Note that we want the activation from early layers\n",
    "#Recall from style transfer paper:\n",
    "#   Early conv layers gives clearer contents\n",
    "\n",
    "def get_output(m, ln):\n",
    "    name = 'block' + str(ln) + '_conv1'\n",
    "    return m.get_layer(name).output\n",
    "\n",
    "def get_output_2(m, ln):\n",
    "    name = 'block' + str(ln) + '_conv2'\n",
    "    return m.get_layer(name).output\n",
    "\n",
    "#Define model that will grab the activations from first 3 conv layers\n",
    "vgg_content = Model(vgg_inp, [get_output(vgg, o) for o in [1,2,3]])\n",
    "\n",
    "#Define model that will grab the activations from first 5 conv layers \n",
    "vgg_style = Model(vgg_inp, [get_output_2(vgg, o) for o in [1,2,3,4,5]])\n",
    "\n",
    "#vgg1 = for the high res image\n",
    "vgg1 = vgg_content(vgg_inp)\n",
    "\n",
    "#vgg2 = for the generated image\n",
    "vgg2 = vgg_content(n1_out)\n",
    "\n",
    "#vgg3 = for style image\n",
    "vgg3 = vgg_style(vgg_inp)\n",
    "\n",
    "#vgg4 = for the high res image\n",
    "vgg4 = vgg_style(n1_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(diff): \n",
    "    dims = list(range(1,K.ndim(diff)))\n",
    "    return K.expand_dims(K.sqrt(K.mean(diff**2, dims)), 0)\n",
    "\n",
    "def gram_matrix(x):\n",
    "    features = []\n",
    "    for i in range (len(x)):\n",
    "        features.append(K.batch_flatten(K.permute_dimensions(x[i], (0,3,1,2))))\n",
    "    grams = []\n",
    "    for i in range(len(x)):\n",
    "        num_el = x[i].shape[1].value * x[i].shape[2].value * x[i].shape[3].value\n",
    "        num_el = num_el * 1.0\n",
    "        numerator = K.dot(features[i], K.transpose(features[i]))\n",
    "        res = numerator/num_el\n",
    "        grams.append(res)\n",
    "    return grams\n",
    "\n",
    "layer_weights=[0.3, 0.65, 0.05]\n",
    "def content_fn(x):\n",
    "    res = 0\n",
    "    n=len(layer_weights)\n",
    "    for i in range(n):\n",
    "        res += (mean_squared_error(x[i]-x[i+n]) * layer_weights[i])*0.975\n",
    "    return res\n",
    "\n",
    "style_weights = [0.1,0.2,0.2,0.25,0.25]\n",
    "def style_fn(x):\n",
    "    res = 0\n",
    "    n=len(style_weights)\n",
    "    grams = gram_matrix(x) #list of 10 gram matrices\n",
    "    for i in range(n):\n",
    "        gram1 = grams[i]\n",
    "        gram2 = grams[i+n]\n",
    "        res += (mean_squared_error(gram1-gram2) * style_weights[i])*0.025\n",
    "    return res\n",
    "\n",
    "def loss_fn(x):\n",
    "    content = content_fn(x)\n",
    "    style = style_fn(x)\n",
    "    total = content + style\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_loss = (Lambda(content_fn)(vgg1+vgg2))\n",
    "style_loss = (Lambda(style_fn)(vgg3+vgg4))\n",
    "total_loss = add([style_loss,content_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define the model that actually minimizes the loss\n",
    "#model = Model([inp, vgg_inp], (Lambda(content_fn)(vgg1+vgg2)) + (Lambda(style_fn)(vgg3+vgg4)))\n",
    "model = Model([inp, vgg_inp],total_loss)\n",
    "\n",
    "#We want the output of our model (loss) to be zeros\n",
    "target = np.zeros((num_images, 1))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model_style.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "33/33 [==============================] - 33s - loss: 41136858.3030    \n",
      "Epoch 2/200\n",
      "33/33 [==============================] - 30s - loss: 13109994.4242    \n",
      "Epoch 3/200\n",
      "33/33 [==============================] - 30s - loss: 5198462.0720    \n",
      "Epoch 4/200\n",
      "33/33 [==============================] - 30s - loss: 3377108.5114    \n",
      "Epoch 5/200\n",
      "33/33 [==============================] - 30s - loss: 2697655.2746    \n",
      "Epoch 6/200\n",
      "33/33 [==============================] - 30s - loss: 1792765.7008    \n",
      "Epoch 7/200\n",
      "33/33 [==============================] - 30s - loss: 1606019.3485    \n",
      "Epoch 8/200\n",
      "33/33 [==============================] - 30s - loss: 990831.4787     \n",
      "Epoch 9/200\n",
      "29/33 [=========================>....] - ETA: 3s - loss: 538215.2565"
     ]
    }
   ],
   "source": [
    "model.fit([x_train, y_train], target, batch_size=1, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=10\n",
    "\n",
    "#Define the trained model\n",
    "trained_model = Model(inp, n1_out)\n",
    "\n",
    "#Let's predict the first 3 images in our dataset\n",
    "predictions = trained_model.predict(x_train[:num])\n",
    "\n",
    "i = 0\n",
    "#single_prediction = predictions[i].astype('uint8')\n",
    "#imsave('before.jpg',x_train[i])\n",
    "#imsave('after.jpg',single_prediction)\n",
    "\n",
    "columns = ['{}'.format(col) for col in ['Before', 'After', 'Ground Truth']]\n",
    "\n",
    "fig,axis = plt.subplots(nrows=num, ncols=3, figsize=(22,22))\n",
    "\n",
    "for ax,col in zip(axis[0], columns):\n",
    "    ax.set_title(col)\n",
    "\n",
    "for i in range(num):\n",
    "    axis[i,0].imshow(Image.fromarray(x_train[i].astype('uint8')))\n",
    "    axis[i,1].imshow(Image.fromarray(predictions[i].astype('uint8')))\n",
    "    axis[i,2].imshow(Image.fromarray(y_train[i].astype('uint8')))\n",
    "    axis[i,0].set_yticklabels([])\n",
    "    axis[i,0].set_xticklabels([])\n",
    "    axis[i,1].set_yticklabels([])\n",
    "    axis[i,1].set_xticklabels([])\n",
    "    axis[i,2].set_yticklabels([])\n",
    "    axis[i,2].set_xticklabels([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im = Image.open('before.jpg')\n",
    "im = im.resize((720,480))\n",
    "im = np.asarray(im)\n",
    "im = np.expand_dims(im,axis=0)\n",
    "p = trained_model.predict(im)\n",
    "\n",
    "gimp = Image.open('after_GIMP_autoenhance.jpg')\n",
    "gimp = np.asarray(gimp)\n",
    "gimp = np.expand_dims(gimp,axis=0)\n",
    "\n",
    "post = Image.open('after_post.jpg')\n",
    "post = np.asarray(post)\n",
    "post = np.expand_dims(post,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imsave('after.jpg',p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(nrows=2, ncols=2, figsize=(20,20))\n",
    "axes[0,0].set_title('Before',fontsize=20)\n",
    "axes[0,0].imshow(im[0])\n",
    "axes[0,0].set_yticklabels([])\n",
    "axes[0,0].set_xticklabels([])\n",
    "axes[0,1].set_title('Neural Autoenhance',fontsize=20)\n",
    "axes[0,1].imshow(p[0].astype('uint8'))\n",
    "axes[0,1].set_yticklabels([])\n",
    "axes[0,1].set_xticklabels([])\n",
    "axes[1,0].set_title('GIMP Autoenhance',fontsize=20)\n",
    "axes[1,0].imshow(gimp[0].astype('uint8'))\n",
    "axes[1,0].set_yticklabels([])\n",
    "axes[1,0].set_xticklabels([])\n",
    "axes[1,1].set_title('Neural Autoenhance Postprocessed (Brightness + Contrast)',fontsize=20)\n",
    "axes[1,1].imshow(post[0].astype('uint8'))\n",
    "axes[1,1].set_yticklabels([])\n",
    "axes[1,1].set_xticklabels([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trained_model.save_weights('model32_style_experimental.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
